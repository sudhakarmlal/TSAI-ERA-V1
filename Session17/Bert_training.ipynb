{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d027255e-8c88-471e-ac9c-53588aa5fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6346e93c-4e5f-4832-8c77-84999e7ea088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'training.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1953f5e-7282-4771-916f-6d89337d29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3b62ac-19ce-4129-a09a-520849ca8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SentencesDataset\n",
    "train_ds = SentencesDataset(sentences,vocab,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216a614f-5de9-4f72-bc91-32636876ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(train_ds, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79275a21-8e36-46b5-94ba-0a1f2df6f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873233bb-2f11-4b04-a1bc-1595fc39b38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23948"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b503540-f752-425f-983b-e76a73caa35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(name = \"BERT\",n_code=8, n_heads=8, embed_size=128, inner_ff_size = 128*4, n_embeddings=len(train_ds.vocab), seq_len = 20, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b05538-c9ad-4144-9519-032d12c8fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b69838-ec5d-4a73-ab4f-20d9aee99103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(name='BERT', n_code=8, n_heads=8, embed_size=128, inner_ff_size=512, n_embeddings=23948, seq_len=20, dropout=0.1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e3f3b17-e0f8-4006-bd21-effe51082643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "from dataset import get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71a27e7d-609f-400e-853e-558bc1fd578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f8a7539-ad29-424c-8d6d-c6c6ea08860e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing optimizer and loss...\n",
      "training...\n",
      "it: 0  | loss 10.36  | Δw: 2.46\n",
      "it: 10  | loss 9.82  | Δw: 1.737\n",
      "it: 20  | loss 9.63  | Δw: 1.279\n",
      "it: 30  | loss 9.43  | Δw: 1.197\n",
      "it: 40  | loss 9.35  | Δw: 1.086\n",
      "it: 50  | loss 8.98  | Δw: 0.923\n",
      "it: 60  | loss 9.03  | Δw: 0.748\n",
      "it: 70  | loss 8.78  | Δw: 0.804\n",
      "it: 80  | loss 8.75  | Δw: 0.901\n",
      "it: 90  | loss 8.81  | Δw: 0.754\n",
      "saving embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19913"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=train_ds.IGNORE_IDX)\n",
    "\n",
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 100\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    \n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output,loss = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss \n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "     \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.transformer.wte.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.transformer.wte.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [train_ds.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5eba59-6b3c-46c3-82d8-5effd2009643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
